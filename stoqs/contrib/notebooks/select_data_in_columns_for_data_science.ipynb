{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Data in Columns for Data Science\n",
    "*Pivot the row-based data in a STOQS database to fit into a column-based dataframe*\n",
    "\n",
    "This Notebook explores options raised by this [GitHub Issue](https://github.com/stoqs/stoqs/issues/837#issuecomment-763176111).\n",
    "\n",
    "Executing this Notebook requires a personal STOQS server.  It can be run from either a Docker installation or from a development Vagrant Virtual Machine. \n",
    "\n",
    "### Docker Instructions\n",
    "Install and start the software as \n",
    "[detailed in the README](https://github.com/stoqs/stoqs#production-deployment-with-docker). (Note that on MacOS you will need to modify settings in your `docker-compose.yml` and `.env` files &mdash; look for comments referencing 'HOST_UID'.)\n",
    "        \n",
    "Then, from your `$STOQS_HOME/docker` directory start the Jupyter Notebook server pointing to MBARI's master STOQS database server. Note: firewall rules limit unprivileged access to such resources. Alternately, you may not set DATABASE_URL and use a local database on your system.\n",
    "\n",
    "    docker-compose exec \\\n",
    "        -e DATABASE_URL=postgis://everyone:guest@kraken.shore.mbari.org:5432/stoqs \\\n",
    "        stoqs stoqs/manage.py shell_plus --notebook\n",
    "\n",
    "A message is displayed giving a URL for you to use in a browser on your host, e.g.:\n",
    "\n",
    "    http://127.0.0.1:8888/?token=<a_token_generated_upon_server_start>\n",
    "\n",
    "In the browser window opened to this URL navigate to this file (`stoqs/contrib/notebooks/plot_by_standard_name.ipynb`) and open it. You will then be able to execute the cells and modify the code to suit your needs.\n",
    "\n",
    "---\n",
    "\n",
    "### Vagrant VM Instructions\n",
    "Install and provision your VM as [detailed in the README](https://github.com/stoqs/stoqs#getting-started-with-a-stoqs-development-system) and configure to use MBARI's campaigns:\n",
    "\n",
    "    cd $STOQS_HOME/stoqs\n",
    "    ln -s mbari_campaigns.py campaigns.py\n",
    "\n",
    "Launch the Jupyter Notebook server on your VM using MBARI's master STOQS database server:\n",
    "\n",
    "    cd $STOQS_HOME/stoqs/contrib/notebooks\n",
    "    DATABASE_URL=postgis://everyone:guest@kraken.shore.mbari.org:5432/stoqs \\\n",
    "        ../../manage.py shell_plus --notebook\n",
    "        \n",
    "(Note: firewall rules limit unprivileged access to such resources. Alternately, you may not set DATABASE_URL and use a local database on your VM â€“ note that postgres needs more than the default 2 GB RAM given by Docker Desktop, increasing it to at least 16 GB seems to help with the `.read_sql_query()` calls.)\n",
    "\n",
    "A message is displayed giving a URL for you to use in a browser on your host, e.g.:\n",
    "\n",
    "    http://127.0.0.1:8888/?token=<a_token_generated_upon_server_start>\n",
    "\n",
    "Port 8888 on your Vagrant VM is mapped to port 8887 on your host, so in a web browser on your host open the URL (using the `<a_token_generated_upon_server_start>` printed after the Jupyter Notebook server is started):\n",
    "\n",
    "    http://127.0.0.1:8887/?token=<a_token_generated_upon_server_start>\n",
    "\n",
    "Navigate to this file (stoqs/contrib/notebooks/Select_Data_in_Columns_for_Data_Science.ipynb) and open it. You will then be able to execute the cells and modify the code to suit your needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Prevent SynchronousOnlyOperation exceptions\n",
    "os.environ[\"DJANGO_ALLOW_ASYNC_UNSAFE\"] = \"true\"\n",
    "\n",
    "# Use a recent database available at DATABASE_URL\n",
    "db = 'stoqs_canon_october2020'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. Perform a straight forward query using the STOQS data model, collecting all the sea_water_temperature and sea_water_salinity data into dictionaries keyed by platform name. This is to examine the landscape of data we are querying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To make sure we collect temperatures and salinities that are properly associated\n",
    "# we will first find all the Platforms that have T & S and then from each Measurement\n",
    "# from the Platform collect the temperatures and salinities into lists for plotting.\n",
    "# Assume that Platforms that have sea_water_salinity also have sea_water_temperature.\n",
    "platforms = (ActivityParameter.objects.using(db)\n",
    "                              .filter(parameter__standard_name='sea_water_salinity')\n",
    "                              .values_list('activity__platform__name', flat=True)\n",
    "                              .distinct().order_by('activity__platform__name'))\n",
    "temps = {}\n",
    "salts = {}\n",
    "for platform in platforms:\n",
    "    print(f\"Collecting data for: {platform:23}\", end=' ')\n",
    "    mps = (MeasuredParameter.objects.using(db)\n",
    "           .filter(measurement__instantpoint__activity__platform__name=platform))\n",
    "    \n",
    "    temps[platform] = (mps.filter(parameter__standard_name='sea_water_temperature')\n",
    "                          .values_list('datavalue', flat=True))\n",
    "    salts[platform] = (mps.filter(parameter__standard_name='sea_water_salinity')\n",
    "                          .values_list('datavalue', flat=True))\n",
    "    print(f\"#temps: {len(temps[platform]):6}  #salts: {len(salts[platform]):6}\", end='')\n",
    "    if len(temps[platform]) != len(salts[platform]):\n",
    "        print(' - not equal')\n",
    "    else:\n",
    "        print()\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a T/S plots of data from all the platforms\n",
    "\n",
    "import pylab as plt\n",
    "for platform in temps.keys():\n",
    "    ##print(f\"Plotting data from {platform}\")\n",
    "    if len(temps[platform]) == len(salts[platform]):\n",
    "        plt.scatter(temps[platform], salts[platform])\n",
    "        plt.title(platform)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Use the same kind of self-join query used for selecting data for Parameter-Parameter plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_multp = '''SELECT DISTINCT stoqs_measuredparameter.id,\n",
    "                stoqs_platform.name,\n",
    "                stoqs_measurement.depth,\n",
    "                mp_salt.datavalue AS salt,\n",
    "                mp_temp.datavalue AS temp\n",
    "FROM stoqs_measuredparameter\n",
    "INNER JOIN stoqs_measurement ON (stoqs_measuredparameter.measurement_id = stoqs_measurement.id)\n",
    "INNER JOIN stoqs_instantpoint ON (stoqs_measurement.instantpoint_id = stoqs_instantpoint.id)\n",
    "INNER JOIN stoqs_activity ON (stoqs_instantpoint.activity_id = stoqs_activity.id)\n",
    "INNER JOIN stoqs_platform ON (stoqs_activity.platform_id = stoqs_platform.id)\n",
    "INNER JOIN stoqs_measurement m_salt ON m_salt.instantpoint_id = stoqs_instantpoint.id\n",
    "INNER JOIN stoqs_measuredparameter mp_salt ON mp_salt.measurement_id = m_salt.id\n",
    "INNER JOIN stoqs_parameter p_salt ON mp_salt.parameter_id = p_salt.id\n",
    "INNER JOIN stoqs_measurement m_temp ON m_temp.instantpoint_id = stoqs_instantpoint.id\n",
    "INNER JOIN stoqs_measuredparameter mp_temp ON mp_temp.measurement_id = m_temp.id\n",
    "INNER JOIN stoqs_parameter p_temp ON mp_temp.parameter_id = p_temp.id\n",
    "WHERE (p_salt.standard_name = 'sea_water_temperature')\n",
    "  AND (p_temp.standard_name = 'sea_water_salinity')\n",
    "  AND stoqs_platform.name IN ({})'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"DJANGO_ALLOW_ASYNC_UNSAFE\"] = \"true\"\n",
    "db = 'stoqs_canon_october2020'\n",
    "platforms = (ActivityParameter.objects.using(db)\n",
    "                              .filter(parameter__standard_name='sea_water_salinity')\n",
    "                              .values_list('activity__platform__name', flat=True)\n",
    "                              .distinct())\n",
    "plats = ''\n",
    "for platform in platforms:\n",
    "    ##if platform == 'makai' or platform == 'pontus':\n",
    "    ##    continue\n",
    "    plats += f\"'{platform}',\"\n",
    "plats = plats[:-2] + \"'\"\n",
    "sql = sql_multp.format(plats)\n",
    "print(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from django.db import connections\n",
    "%time df = pd.read_sql_query(sql, connections[db])\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It takes about 2 minutes to read 25 million rows from the STOQS database. Let's save this in Apache Parquet format and then test loading that for our data visualization needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# See: https://datashader.org/\n",
    "import datashader as ds, pandas as pd, colorcet\n",
    "\n",
    "cvs = ds.Canvas(plot_width=300, plot_height=300)\n",
    "agg = cvs.points(df.loc[df['name'].isin(('pontus','makai'))], 'temp', 'salt')\n",
    "img = ds.tf.shade(agg, cmap=colorcet.fire, how='eq_hist')\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See: https://datashader.org/getting_started/Pipeline.html\n",
    "import holoviews as hv\n",
    "from holoviews.operation.datashader import datashade\n",
    "hv.extension(\"bokeh\")\n",
    "datashade(hv.Points(df, kdims=['temp', 'salt']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See: http://holoviews.org/user_guide/Large_Data.html\n",
    "from holoviews.operation.datashader import rasterize\n",
    "##ropts = dict(tools=[\"pan,wheel_zoom,box_zoom\"], height=380, width=330, colorbar=True, colorbar_position=\"bottom\")\n",
    "ropts = dict(height=380, width=330, colorbar=True, colorbar_position=\"bottom\")\n",
    "%time hv.Layout([rasterize(hv.Points(df.loc[df['name'] == p],kdims=['temp', 'salt'])).opts(**ropts).relabel(p)for p in platforms])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WIP below this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#See: https://stackoverflow.com/questions/50755586/how-to-loop-large-parquet-file-with-generators-in-python\n",
    "from fastparquet import ParquetFile\n",
    "pf = ParquetFile('myfile.parq')\n",
    "for df in pf.iter_row_groups():\n",
    "    process sub-data-frame df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p = df.hvplot.points('temp', 'salt', title=platform,\n",
    "                                datashade=True, dynspread=True, \n",
    "                                frame_height=450)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_sql = '''SELECT\n",
    "\t* \n",
    "FROM\n",
    "\tcrosstab( 'SELECT to_char(stoqs_instantpoint.timevalue, ''YYYY-MM-DD\"T\"HH24:MI:SS\"Z\"'') as timevalue, \n",
    "\t                  stoqs_parameter.name as name, datavalue as datavalue FROM public.stoqs_measuredparameter\n",
    "INNER JOIN stoqs_measurement ON (stoqs_measuredparameter.measurement_id = stoqs_measurement.id)\n",
    "INNER JOIN stoqs_instantpoint ON (stoqs_measurement.instantpoint_id = stoqs_instantpoint.id)\n",
    "INNER JOIN stoqs_activity ON (stoqs_instantpoint.activity_id = stoqs_activity.id)\n",
    "INNER JOIN stoqs_platform ON (stoqs_activity.platform_id = stoqs_platform.id)\n",
    "INNER JOIN stoqs_parameter ON (stoqs_measuredparameter.parameter_id = stoqs_parameter.id)\n",
    "WHERE stoqs_platform.name IN (''makai_ESPmv1_filtering'')\n",
    "ORDER BY stoqs_instantpoint.timevalue, stoqs_parameter.name') \n",
    "AS final_result(\"timevalue\" TEXT, \"temperature (Celsius)\" FLOAT, \"salinity\" FLOAT)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Django Shell-Plus",
   "language": "python",
   "name": "django_extensions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
